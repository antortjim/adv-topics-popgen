---
title: "NGS Assignment Part 1"
author: 'KU id: rnq313'
subtitle: Advanced Topics in Bioinformatics
output: html_document
---

```{r setup, eval = TRUE, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("ggplot2")
library("dplyr")
library("kableExtra")
library("knitr")
theme_set(theme_bw())
```

A function that converts the ASCII values into phredScaled quality scores
```{r}
ascii2integer <- function(x){
  y <- utf8ToInt(x) ## ascii to integer
  Q <- y - 33 #offset
  Q
}
```
A function that converts a fastq quality to probability of sequencing the right base, i.e our degre of belief or trust in the sequencing data

```{r}
q2p <- function(q) {
  p <- 1 - 10 ** (-q / 10)
  return(p)
}
```

Read the mpileup data
```{r}
s <- scan("nice1.mpileup",
          what=list("asdf",12,"sdf",12,"df","sdf"))

chr <- s[[1]]
pos <- s[[2]]
ref <- s[[3]]
nReads <- s[[4]]
basesString <- s[[5]]
asciiString <- s[[6]]
bases <- c("A", "C", "G", "T") %>% as.list
```

Observed data: read bases and their quality.
```{r}
basesChar <- basesString %>% as.list %>% lapply(function(x) strsplit(x, "") %>% unlist)

success_probability <- asciiString %>% as.list %>% lapply(function(x) strsplit(x, "") %>% unlist) %>%
  lapply(function(x) lapply(x, function(y) q2p(ascii2integer(y)) %>% unlist) %>% unlist)
```

Implementation of the Expectation Maximisation algorithm.
An analogy to the classical coin example follows:

In this case, there are 4 different latent states (coin ids) corresponding to the 4 possible underlying genotypes for each position (A C G or T). These are unknown but determine the observed sequencing data.
Moreover, data coming from different sites can be modelled as independent coin tosses from one of the 4 coins (latent genotypes) each time, while data from the same site will be generated by the same coin. 
Finally, the probability distribution followed by the tosses is not specified by a binomial, but instead provided by the quality data.

### Write the likelihood model:


The model builds a likelihood matrix of dimensions M (# sites) x 4 (# latent states) where each column i,k stores the likelihood of the data in position i under state k. This likelihood is computed as the product of the likelihoods of each read base in that position.

$$ P(X_i)  = \sum_{k \in \{A,C,G,T \}}{P(X_i | Z_k)} =  \sum_{k \in \{A,C,G,T \}} \prod_{j=1}^{\text{depth}} {P(X_{i,j} | Z_k)} $$
and the likelihood of the data given the underlying genotype is: (GATK)

$$P(X_i | Z_k) = \begin{cases}
             p = 10^{-Q \over 10}  & \text{if } X_i = Z_k\\
             (1 - p)/3  & \text{if } X_i \neq Z_k
             \end{cases}
$$   

Implement the model without prior
```{r}
data_lik <- function(basesChar, success_probability, pos) {
  
  failure_probability <- lapply(success_probability, function(x) unlist(lapply(x, function(y) unlist((1-y) / 3))))
  
  
  lik <- matrix(ncol = 4, nrow = length(success_probability))

  # for each position or site i 
  for (i in 1:length(basesChar)) {
    # initialize the latent state id
    k <- 1
    # fore each latent state (unknown underlying genotype)
    for (b in bases) {
      # initialize a vector storing the prob of every single read letter assumming
      # latent genotype b
      readWise <- c()
      # for each read letter in site i
      for (j in 1:length(basesChar[[i]])) {
        # if the jth letter in position i agrees with the latent genotype b
        # the probability p is given by success_probability, otherwise it's (1 - p) / 3 
        # encoded in failure probability.
        result <- ifelse(
          basesChar[[i]][j] == b,
          success_probability[[i]][j],
          failure_probability[[i]][j]
          )
        # attach this prob to the vector
        readWise[j] <- result
      } # computed for all data in site i
     
      # the likelihood of all the data in site i under the
      # kth latent genotype (letter bases[k]) is the product of the individual likelihoods
      lik[i, k] <- prod(readWise)
      # update the latent genotype index and repeat for another latent genotype
      k <- k + 1
    } # completed for all possible genotypes
  } # in all sites too! :)

  # llik now stores in the i,k cell the likelihood of the data
  # in site i under genotype bases[k]
  return(lik)
}

```

Once the likelihood matrix is built, the current frequency estimates $\theta^n$ can be multiplied row-wise with the matrix, to get the likelihood of the data given the estimates of the latent genotypes frequencies. This new matrix stores the Q values, or expected likelihoods under the current estimates. These likelihoods can be optimized (maximised) with a new set of frequency estimates $\theta^{n+1}$ that become the output of one iteration of the algorithm.

### Write the Q (estimation) and M step of the EM algorithm that you will need for the optimization
```{r, eval = F, echo = T}
q <- c(theta, 1 - sum(theta)) * lik
```

$$
\begin{aligned}
& q(Z_k) \\
& =  P(X_i, Z_k | \theta) \\
& = P(X_i | Z_k, \theta) \times P(Z_k | \theta) \\
& = P(X_i | Z_k) \times \theta
\end{aligned}
$$ rownames(df) <- c("Null model", "Alt model")



```{r, eval = F, echo = T}
newTheta <- colSums(q) / sum(q)
```

$$
\begin{aligned}
\theta^{n+1}_k = \frac{\sum_i q_i(Z_k)}{\sum_i\sum_k q_i(Z_k)}
\end{aligned}
$$



### Implement the model and estimate the allele frequencies of the four bases
```{r, eval = T, echo = T}
EM_yeast <- function(lik, theta) {
  # EXPECTATION 
  # we can update this likelihoods with our frequency priors
  # by multiplying them row-wise   
  q <- c(theta, 1 - sum(theta)) * lik
 
  # this gives us the expected probability distribution under the
  # hypothesis that the underlying base composition is given by theta
  
  # we can get the log likelihood of this model by
  # summing over all possible genotypes in each site (rowSum)
  # to get the likelihood of the data in each site
  # we can take the log of all the site probabilities and sum them
  # to get a global likelihood of the whole dataset
  loglik <- sum(log(rowSums(q)))
  
  # MAXIMISATION
  # The maximum likelihood estimate of the base frequencies is obtained by summing over columns and normalizing with the whole table sum.
  newTheta <- colSums(q) / sum(q)
  
  return(list(theta = newTheta[1:3],
              loglikelihood = loglik))
}
```

Default estimates (random) and preparation for the algorithm
```{r}
theta <- c(.25, .25, .25)
nIters <- 10
result <- matrix(ncol = 4)
colnames(result) <- bases
result[1,] <- c(theta, 1 - sum(theta))
change <- rep(1, 4)
i <- 1
```

The algorithm must be called repeteadly inside an iterative loop until convergence criteria are satisfied or a max number of iterations are completed.

Repeat until the change in any of the estimates is less than 0.001 or the number of iterations becomes greater than `nIters`
Attach each estimate of $\theta$ to a new row in the result matrix.
```{r}
# Data likelihoods
lik <- data_lik(basesChar, success_probability, pos)
while(any(change > 0.0001) && i < nIters) {
  # Run the EM algorithm
  EM_result <- EM_yeast(lik, theta)
  # Extract the theta
  theta <- EM_result$theta
  current <- c(theta, 1 - sum(theta))
  names(current) <- NULL
  names(current) <- bases
  
  # Attach it to the matrix
  result <- rbind(result, current)
  
  # Measur the change in the theta values
  change <- abs(result[i+1, ] - result[i,])
  i <- i + 1
}

```

Resulting estimation of allele frequencies ($\theta$).
```{r}
kable_data <- as.data.frame(round(result[nrow(result),], 2))
colnames(kable_data) <- "theta"
kable(kable_data, "html") %>% kable_styling()
```

Plot the evolution of the $\theta$ estimates.
```{r}
rownames(result) <- NULL
df <- tidyr::gather(cbind(x = 1:nrow(result), as.data.frame(result)), base, value, -x)

# Make the legend sorted by estimate value
df$base <- factor(df$base, levels = result[nrow(result),] %>% sort %>% rev %>% names)
p <- ggplot(data = df,
            mapping = aes(x = x, y = value, group = base, col = base)) +
  geom_line() +
  scale_x_continuous(breaks = 1:(nIters+1))

# result[nrow(result),]
freqs <- basesChar %>% lapply(function(x) which.max(table(x))) %>% unlist %>% names %>% table
freqs <- freqs / sum(freqs)

q <- p + geom_line(data = data.frame(x = rep(1:nrow(result), each = length(freqs)),
                                y = rep(freqs, times = nrow(result)),
                                base = rep(unlist(bases), times = i)),
              mapping = aes(x = x, y = y, group = base, col = base), linetype = 2)
q <- q + labs(title = "EM algorithm convergence", x = "Steps", y = expression(theta),
              subtitle = "Reaching a maximum likelihood estimate of the base composition") +
  scale_color_brewer(palette = "Set1")
print(q)
```

### What is the GC contents?
```{r}
result[nrow(result),"C"] + result[nrow(result),"G"]
```


### Make a likelihood ratio test that tests whether you can reject that your sample have the same allele frequencies as the other experiment.
### Do you get a signifcantly different results based on your sample?

The null model suggests only one parameter determines the 4 frequencies, given that $f_A = f_T$ and $f_G = f_C$. Therefore, the difference in the number of parameters is 2 (3-1 = 2) 

```{r}
null_freqs <- c((1 - 0.34)/2, 0.34/2, 0.34/2, (1-0.34)/2)
EM_null <- EM_yeast(lik, null_freqs[1:3])
alternative_loglik <- EM_result$loglikelihood
null_loglik <- EM_null$loglikelihood
```

$$
\begin{aligned}
& H_0: \text{GC content is 34 % and G and C contents are equal} \\
& H_1: \text{Base frequencies are given by the Maximum Likelihoods estimates }
\end{aligned}
$$
```{r}
chisq <- 2 * (alternative_loglik - null_loglik)
print(chisq)
pvalue <- 1 - pchisq(q = chisq, df = 2)
print(pvalue)
```

The results of the test show that there's no evidence that $H_0$ is FALSE, hence there's no evidence that the null model is worse than the alternative model. The differences between both models are negligible and thus NOT SIGNIFICANTLY DIFFERENT.