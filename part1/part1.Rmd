---
title: "R Notebook"
output: html_notebook
---
  
  
```{r}
library("ggplot2")
library("dplyr")
theme_set(theme_bw())
setwd("/home/antortjim/MEGA/Master/ATB/Assignments/assignment3")
```

```{r}
## function that converts the ascii values into phredScaled quality scores
ascii2integer <- function(x){
  y <- utf8ToInt(x) ## ascii to integer
  Q <- y - 33 #offset
  Q
}

## function that converts a fastq quality to probability of sequencing the right base
## i.e our degre of belief or trust in the sequencing data
q2p <- function(q) {
  p <- 1 - 10 ** (-q / 10)
  return(p)
}
```

Read the mpileup data
```{r}
s <- scan("nice1.mpileup",
          what=list("asdf",12,"sdf",12,"df","sdf"))

chr <- s[[1]]
pos <- s[[2]]
ref <- s[[3]]
nReads <- s[[4]]
basesString <- s[[5]]
asciiString <- s[[6]]

bases <- c("A", "C", "G", "T") %>% as.list
```

Observed data: read bases and their quality
```{r}
basesChar <- basesString %>% as.list %>% lapply(function(x) strsplit(x, "") %>% unlist)

success_probability <- asciiString %>% as.list %>% lapply(function(x) strsplit(x, "") %>% unlist) %>%
  lapply(function(x) lapply(x, function(y) q2p(ascii2integer(y)) %>% unlist) %>% unlist)
```

Implementation of the Expectation Maximisation algorithm.
An analogy to the classical coin example follows:

In this case, there are 4 different latent states (coin ids) corresponding to the 4 possible underlying genotypes for each position (A C G or T). These are unknown but determine the observed sequencing data.
Moreover, data coming from different sites can be modelled as independent coin tosses from one of the 4 available ones each time. But data from the same site will be generated by the same coin (underlying latent variable or genotype).
Finally, the probability distribution followed by the tosses is not specified by a binomial, but instead provided by the quality data.

The probability that the underlying genotype agrees with the read data is $p = 10^{-Q \over 10}$, and ${1 - p} \over 3$ otherwise.

The model builds a likelihood matrix of dimensions M (# sites) x 4 (# latent states) where each column m,z stores the log likelihood of the data in position m under state z. This likelihood is computed as the sum of the log likelihoods of each read base in that position.

Once the log likelihood matrix is built, the logarithm of the current frequency estimates $\theta^n$ can be added rowwise to the matrix, to get the likelihood of the data given the estimates of the hidden states frequencies. This new matrix stores the Q values, or expected likelihoods under the current estimates. These likelihoods can be optimized (maximised) with a new set of frequency estimates $\theta^{n+1}$ that are the output of one iteration of the algorithm.

The algorithm must be call repeteadly inside an interative loop until convergence criteria are satisfied or a max number of iterations are completed.

```{r}
EM_yeast <- function(basesChar, success_probability, pos, theta) {
  
  # for each possible base b
  llik <- lapply(bases, function(b) {
   
    # for each position i 
    1:length(basesChar) %>% as.list %>% lapply(function(i) {
      # for each sequenced base j in that position
      1:length(basesChar[[i]]) %>% as.list %>% lapply(function(j) {
        # if the letter jth letter in position i is equal to the current possible base b
        # the probability p is given by success_probability, otherwise it's (1 - p) / 3 
        ifelse(
          basesChar[[i]][j] == b,
          success_probability[[i]][j],
          (1 - success_probability[[i]][j]) / 3
          )
      }) %>%
        #  sum the logarithm of the probabilities
        unlist %>% log %>% sum
        # this sum is equal to the log likelihood of the data in position i assuming the true base is b
    }) %>%
      # this series of number is the log likelihoods assuming the true base is b for all sites
      unlist
  }) %>%
    # the same but for all possible bases.
    # Fill a matrix with these 4 series of 4397 numbers by cols
    unlist %>% matrix(ncol = 4, byrow = FALSE)
  
  # Name the columns with the bases names (b)
  colnames(llik) <- bases
  # Name the rows with the position indices (i)
  rownames(llik) <- pos
  
  # logq
  lq <- llik + log(c(theta, 1 - sum(theta)))
  # lq <- log(exp(lq) / rowSums(exp(lq)))
  q <- exp(lq)
  newTheta <- colSums(q) / sum(q)
  
  return(list(theta = newTheta[1:3],
              loglikelihood = lq))
}
```


Default estimates (random) and preparation for the algorithm
```{r}
theta <- c(.1, .1, .4)
nIters <- 10
result <- matrix(ncol = 4)
colnames(result) <- bases
result[1,] <- c(theta, 1 - sum(theta))
change <- rep(1, 4)
i <- 1
```
Repeat until the change in any of the estimates is less than 0.001 or the number of iterations becomes greater than `nIters`
Add each estimate of $\theta$ to a new row in the result matrix.
```{r}
while(any(change > 0.001) && i < nIters) {
  EM_result <- EM_yeast(basesChar, success_probability, pos = 1:length(basesChar), theta)
  theta <- EM_result$theta
  current <- c(theta, 1 - sum(theta))
  names(current) <- NULL
  names(current) <- bases
  result <- rbind(result, current)
  change <- abs(result[i+1, ] - result[i,])
  i <- i + 1
}
rownames(result) <- NULL
df <- tidyr::gather(cbind(x = 1:nrow(result), as.data.frame(result)), base, value, -x)
```

Plot the evolution of the $\theta$ estimates.
```{r}
p <- ggplot(data = df,
            mapping = aes(x = x, y = value, group = base, col = base)) +
  geom_line() +
  scale_x_continuous(breaks = 1:(nIters+1))

# result[nrow(result),]
freqs <- basesChar %>% lapply(function(x) which.max(table(x))) %>% unlist %>% names %>% table
freqs <- freqs / sum(freqs)

q <- p + geom_line(data = data.frame(x = rep(1:nrow(result), each = length(freqs)),
                                y = rep(freqs, times = nrow(result)),
                                base = rep(unlist(bases), times = length(bases))),
              mapping = aes(x = x, y = y, group = base, col = base), linetype = 2)
print(q)
```

Estimated GC Content
```{r}
result[nrow(result),"C"] + result[nrow(result),"G"]
```

The alternative model suggests only one parameter determines the 4 frequencies, given that $f_A = f_T$ and $f_G = f_C$. Therefore, the difference in the number of parameters is 2 (3-1 = 2) 
```{r}
alternative_freqs <- c((1 - 0.34)/2, 0.34/2, 0.34/2, (1-0.34)/2)
EM_alternative <- EM_yeast(basesChar, success_probability, pos = 1:length(basesChar), alternative_freqs[1:3])
null_likelihood <- sum(EM_result$loglikelihood)
alternative_likelihood <- sum(EM_alternative$loglikelihood)
```

 
```{r}
chi = 2 * (alternative_likelihood - null_likelihood)
pvalue <- 1 - pchisq(q = chi, df = 2)
print(pvalue)
q <- p + geom_line(data = data.frame(x = rep(1:nrow(result), each = length(alternative_freqs)),
                                y = rep(freqs, times = nrow(result)),
                                base = rep(unlist(bases), times = length(bases))),
              mapping = aes(x = x, y = y, group = base, col = base), linetype = 2)
```

```{r, echo = F}
print(q)
```
This result suggests that the differences between both models are negligible 